{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f923bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#%reload_ext tensorboard\n",
    "#log_folder = './logs9'\n",
    "#writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "223cba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    data_dir = './Data'\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "    [transforms.Resize((300,300)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5],\n",
    "                         std=[0.5])])\n",
    "\n",
    "    trainSet = datasets.ImageFolder(data_dir + '/TrainSet', transform=transform)\n",
    "    print(len(trainSet))\n",
    "    validSet = datasets.ImageFolder(data_dir + '/ValidSet', transform=transform)\n",
    "    testSet = datasets.ImageFolder(data_dir + '/TestSet', transform=transform)\n",
    "\n",
    "    train = DataLoader(trainSet, batch_size=32, shuffle=True)\n",
    "    valid = DataLoader(validSet, batch_size=32, shuffle=False)\n",
    "    test = DataLoader(testSet, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train, valid,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90561fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "def train_imshow():\n",
    "    classes = ('Bad', 'Good')\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    fig, axes = plt.subplots(figsize=(10, 4), ncols=5)\n",
    "    for i in range(5):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i].permute(1, 2, 0))\n",
    "        ax.title.set_text(' '.join('%5s' % classes[labels[i]]))\n",
    "    plt.show()\n",
    "\n",
    "train_loader, val_loader,test_loader = get_data()\n",
    "train_imshow()\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7708bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels= 64, kernel_size= 11, stride=4, padding=0 )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride= 1, padding= 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride= 1, padding= 1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1  = nn.Linear(in_features= 8192, out_features= 4096)\n",
    "        self.fc2  = nn.Linear(in_features= 4096, out_features= 1024)\n",
    "        self.fc3 = nn.Linear(in_features=1024 , out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.maxpool(x)\n",
    "        #print(\"bf flattening: \",x.shape)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        #print(\"after flattening: \",x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8e321a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\"\n",
    "model = Net().to(device)\n",
    "model.train()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4f059a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, train accuracy : 0.5261548757553101, train loss : 0.6931143999099731\n",
      "Epoch : 2, train accuracy : 0.527683436870575, train loss : 0.6930818557739258\n",
      "Epoch : 3, train accuracy : 0.5261548757553101, train loss : 0.6929404735565186\n",
      "Epoch : 4, train accuracy : 0.523097813129425, train loss : 0.6929419040679932\n",
      "Epoch : 5, train accuracy : 0.5261548757553101, train loss : 0.692877471446991\n",
      "Epoch : 6, train accuracy : 0.5261548757553101, train loss : 0.6927440762519836\n",
      "Epoch : 7, train accuracy : 0.5215693116188049, train loss : 0.6928196549415588\n",
      "Epoch : 8, train accuracy : 0.5261548757553101, train loss : 0.6926969885826111\n",
      "Epoch : 9, train accuracy : 0.5292119383811951, train loss : 0.6925444602966309\n",
      "Epoch : 10, train accuracy : 0.53074049949646, train loss : 0.69244384765625\n",
      "Epoch : 11, train accuracy : 0.52004075050354, train loss : 0.6927509903907776\n",
      "Epoch : 12, train accuracy : 0.527683436870575, train loss : 0.6924931406974792\n",
      "Epoch : 13, train accuracy : 0.5322690010070801, train loss : 0.6922557353973389\n",
      "Epoch : 14, train accuracy : 0.53074049949646, train loss : 0.6922302842140198\n",
      "Epoch : 15, train accuracy : 0.53074049949646, train loss : 0.6921827793121338\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss =0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    for data,label in train_loader:\n",
    "        data= data.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        label = label.unsqueeze(-1)\n",
    "        label = label.to(torch.float32)\n",
    "        loss = criterion(output,label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = ((output.argmax(dim=1)==label).float().mean())\n",
    "        epoch_accuracy += acc/len(train_loader)\n",
    "        epoch_loss += loss/len(train_loader)\n",
    "        \n",
    "    print('Epoch : {}, train accuracy : {}, train loss : {}'.format(epoch+1, epoch_accuracy,epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fb3039e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4899],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4899],\n",
      "        [0.4900],\n",
      "        [0.4899],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4900]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Accuracy of the network on the validation images: 53 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        print(outputs)\n",
    "        predicted_targets = outputs.argmax(dim=1)\n",
    "        print(predicted_targets)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted_targets == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the validation images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a10853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
